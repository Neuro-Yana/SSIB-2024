{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fc1134",
   "metadata": {},
   "source": [
    "# SSIB analysis walk through\n",
    "## To DO 0\n",
    "1. Create a git repo\n",
    "2. After each to do, commit and push your changes to the repo (I am going to look at this analysis using the git repo, I should see updates in this notebook at each commit corresponding to the to do)\n",
    "3. Add the Rmd to the git repo as well. THE GIT REPO SHOULD NOT HAVE ANY DATA IN IT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157991cb",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Features\n",
    "The X dataframe is your \"features\". Right now there are too many features for the number of subjects. A good rule of thumb is your features should be 10% or less than your number of samples (subjects). Since we are going to be doing training and testing, we need to limit the number of features to our final testing dataset\n",
    "## Samples\n",
    "This is the number of subjects in our dataset\n",
    "## Targets\n",
    "This is what we are trying to predict. Right now we are trying to predict the high and low SSB groups based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4afd7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  scipy.signal.signaltools\n",
    "\n",
    "def _centered(arr, newsize):\n",
    "    # Return the center newsize portion of the array.\n",
    "    newsize = np.asarray(newsize)\n",
    "    currsize = np.array(arr.shape)\n",
    "    startind = (currsize - newsize) // 2\n",
    "    endind = startind + newsize\n",
    "    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n",
    "    return arr[tuple(myslice)]\n",
    "\n",
    "scipy.signal.signaltools._centered = _centered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9fa766a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'alert_future_error' from 'nibabel.deprecated' (/Users/yana/opt/anaconda3/lib/python3.9/site-packages/nibabel/deprecated.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#import nibabel as nib\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#from nilearn.input_data import NiftiLabelsMasker\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#from nilearn.plotting import plot_glass_brain, plot_stat_map, view_img, view_img_on_surf\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Brain_Data, Adjacency\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roi_to_brain, expand_mask\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fdr, threshold\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltools/__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m ]\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Roc\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_cv\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Brain_Data, Adjacency, Groupby, Design_Matrix, Design_Matrix_Series\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltools/analysis.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_plot\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm, binom_test\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auc\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltools/plotting.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fft, fftfreq\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m two_sample_permutation, one_sample_permutation\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_glass_brain, plot_stat_map, view_img, view_img_on_surf\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprefs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNI_Template, resolve_mni_path\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltools/stats.py:62\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp1d\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/__init__.py:42\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm99analyze \u001b[38;5;28;01mas\u001b[39;00m spm99\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm2analyze \u001b[38;5;28;01mas\u001b[39;00m spm2\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nifti1 \u001b[38;5;28;01mas\u001b[39;00m ni1\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ecat\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/imagestats.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"Functions for computing image statistics\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimageclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spatial_axes_first\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_nonzero_voxels\u001b[39m(img):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Count number of non-zero voxels\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/imageclasses.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyze\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnalyzeImage\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrikhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AFNIImage\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cifti2Image\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataobj_images\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataobjImage\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilebasedimages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileBasedImage\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/cifti2/__init__.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# emacs: -*- mode: python-mode; py-indent-offset: 4; indent-tabs-mode: nil -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# vi: set ft=python sts=4 ts=4 sw=4 et:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"CIFTI-2 format IO\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m.. currentmodule:: nibabel.cifti2\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m   cifti2_axes\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     CIFTI_BRAIN_STRUCTURES,\n\u001b[1;32m     22\u001b[0m     CIFTI_MODEL_TYPES,\n\u001b[1;32m     23\u001b[0m     Cifti2BrainModel,\n\u001b[1;32m     24\u001b[0m     Cifti2Header,\n\u001b[1;32m     25\u001b[0m     Cifti2HeaderError,\n\u001b[1;32m     26\u001b[0m     Cifti2Image,\n\u001b[1;32m     27\u001b[0m     Cifti2Label,\n\u001b[1;32m     28\u001b[0m     Cifti2LabelTable,\n\u001b[1;32m     29\u001b[0m     Cifti2Matrix,\n\u001b[1;32m     30\u001b[0m     Cifti2MatrixIndicesMap,\n\u001b[1;32m     31\u001b[0m     Cifti2MetaData,\n\u001b[1;32m     32\u001b[0m     Cifti2NamedMap,\n\u001b[1;32m     33\u001b[0m     Cifti2Parcel,\n\u001b[1;32m     34\u001b[0m     Cifti2Surface,\n\u001b[1;32m     35\u001b[0m     Cifti2TransformationMatrixVoxelIndicesIJKtoXYZ,\n\u001b[1;32m     36\u001b[0m     Cifti2VertexIndices,\n\u001b[1;32m     37\u001b[0m     Cifti2Vertices,\n\u001b[1;32m     38\u001b[0m     Cifti2Volume,\n\u001b[1;32m     39\u001b[0m     Cifti2VoxelIndicesIJK,\n\u001b[1;32m     40\u001b[0m     load,\n\u001b[1;32m     41\u001b[0m     save,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2_axes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axis, BrainModelAxis, LabelAxis, ParcelsAxis, ScalarAxis, SeriesAxis\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_cifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cifti2Extension\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/cifti2/cifti2.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataobj_images\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataobjImage\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilebasedimages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileBasedHeader, SerializableImage\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnifti1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nifti1Extensions\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nifti2Header, Nifti2Image\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Recoder, make_dt_codes\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nibabel/nifti1.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatteryrunners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Report\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m have_binary128\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alert_future_error\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilebasedimages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFileError, SerializableImage\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptpkg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optional_package\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'alert_future_error' from 'nibabel.deprecated' (/Users/yana/opt/anaconda3/lib/python3.9/site-packages/nibabel/deprecated.py)"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "#import nilearn\n",
    "import numpy as np\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import rankdata, ttest_rel, ttest_1samp\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#import nibabel as nib\n",
    "#from nilearn.input_data import NiftiLabelsMasker\n",
    "#from nilearn.plotting import plot_glass_brain, plot_stat_map, view_img, view_img_on_surf\n",
    "\n",
    "from nltools.data import Brain_Data, Adjacency\n",
    "from nltools.mask import roi_to_brain, expand_mask\n",
    "from nltools.stats import fdr, threshold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e960d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aa12aca",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Change the path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57f201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basepath = '/Users/gracer/Library/CloudStorage/OneDrive-UniversityofWyoming/0. Lab/M2AENAD Lab - Documents/RESEARCH/ABCD/Yana_SSIB_2024/'\n",
    "basepath = '/Users/yana/Library/CloudStorage/OneDrive-UniversityofWyoming/Desktop - Copy/Lab/SSIB 2024/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f653f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(basepath,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatchedFinal.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(basepath,'data','matchedFinal.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78721282",
   "metadata": {},
   "source": [
    "# To DO 1\n",
    "1. Get the number of subjects call them 'n'\n",
    "2. Get a list of ROIs that include all the brain regions, sex, bmi_percentile, household income, and age  \n",
    "**Call this list of ROIs \"ROIS\"**  \n",
    "3. Set sex to 0 for M and 1 for female  \n",
    "4. Set ssb groups to low =0, medium = 1, and high = 2  \n",
    "5. Drop the medium group  \n",
    "6. Create a dataframe called 'X' that is a subset of the dataframe with only the columns in the ROIS list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the DataFrame to make columns become rows\n",
    "X_T = X.T\n",
    "\n",
    "duplicates = X_T.duplicated(keep='first')\n",
    "# Identifying columns to drop (all duplicates except the first occurrence)\n",
    "cols_to_drop = X_T[duplicates].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ab41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the DataFrame to make columns become rows\n",
    "X_T = X.T\n",
    "duplicates = X_T.duplicated(keep='first')\n",
    "# Identifying columns to drop (all duplicates except the first occurrence)\n",
    "cols_to_drop = X_T[duplicates].index\n",
    "# Drop the duplicate columns from the original DataFrame\n",
    "X_cleaned = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "print(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e779d",
   "metadata": {},
   "source": [
    "## Why do we have to do the step above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a01a5",
   "metadata": {},
   "source": [
    "# To DO 2\n",
    "1. Create a dataframe 'y' with only the targets\n",
    "2. Check the number of each target group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcd96a",
   "metadata": {},
   "source": [
    "# Train and test datasets\n",
    "Get randomly generated train and test datasets\n",
    "- Train 1 = train the model and feature elimination\n",
    "- Train 2 = cross validate the model\n",
    "- Test = test statistical differences\n",
    "-- In Test we will also have trainReg and testReg \n",
    "-- We need to train and test the signifance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f427da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train, y_train, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59d890",
   "metadata": {},
   "source": [
    "## What proportion of data is in:\n",
    "1. X_train1?\n",
    "2. X_train2?\n",
    "3. X_test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed789095",
   "metadata": {},
   "source": [
    "# To DO 3\n",
    "Now we are going run the model. We are going to use an anova filter with a SVC linear kernel. \n",
    "1. Run the pipeline with the following parameters\n",
    "- Make a list of anova__K parameters with a maximum value that is 10% of our X_test sample in intervals of 10\n",
    "For example if our max was 1000 we would have a list from 10 to 1000 by 10 (10, 20, 30...1000)\n",
    "-- call this list 'ANOVAK'\n",
    "- Make a list of svc__C parameters that include 0.1, 1, 10, 100\n",
    "-- call this list 'svcC'\n",
    "2. With your lists run the pipeline below\n",
    "## Why are we using an ANOVA filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS THE PIPELINE ##\n",
    "\n",
    "anova_filter = SelectKBest(f_classif)\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "anova_svm = Pipeline([\n",
    "    ('anova', anova_filter),\n",
    "    ('svc', svm)\n",
    "])\n",
    "# Define a range of parameters for feature selection and SVM\n",
    "param_grid = {\n",
    "    'anova__k': ANOVAK,  # Trying different numbers of top features\n",
    "    'svc__C': svcC,  # SVM regularization parameter\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(anova_svm, param_grid=param_grid, cv=10, n_jobs=4)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eed021",
   "metadata": {},
   "source": [
    "# To DO 4\n",
    "1. Look at the \"best parameters\". Based on this narrow your ANOVAK and svcC lists. The ANOVAK should have the best value and the 10 digits around it. For example if the best value was 20, the next ANOVAK list should be [15,16,17,18,19,20,21,22,23,24,25]. Do the same with the best value of svcC.\n",
    "2. Run the loop below (this will take a while) with your new parameters\n",
    "## What is the optimum number of features and what is the optimum svc C based on your loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc6244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a range of parameters for feature selection and SVM\n",
    "param_grid = {\n",
    "    'anova__k': ANOVAK,  # Trying different numbers of top features\n",
    "    'svc__C': svcC,  # SVM regularization parameter\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(anova_svm, param_grid=param_grid, cv=10, n_jobs=4)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "bestK = []\n",
    "bestC = []\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    # Setup GridSearchCV\n",
    "    grid_search = GridSearchCV(anova_svm, param_grid=param_grid, cv=10, n_jobs=4)\n",
    "    grid_search.fit(X_train1, y_train1)\n",
    "    bestK.append(grid_search.best_params_['anova__k'])\n",
    "    bestC.append(grid_search.best_params_['svc__C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca39e0e",
   "metadata": {},
   "source": [
    "# To DO 5\n",
    "1. Using the best parameters run the code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0caecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming anova_svm is your original pipeline\n",
    "best_pipeline = Pipeline([\n",
    "    ('anova', SelectKBest(f_classif)),  # You don't need to specify k here; it will be set by best_params_\n",
    "    ('svc', SVC(kernel='linear'))      # No need to specify C here for the same reason\n",
    "])\n",
    "\n",
    "# Set the best parameters found for the entire pipeline\n",
    "best_pipeline.set_params(**grid_search.best_params_)\n",
    "\n",
    "# Now, retrain on the entire training set with the best parameters\n",
    "best_pipeline.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d3cee",
   "metadata": {},
   "source": [
    "# To DO 6\n",
    "1. Get the accuracy score\n",
    "2. Look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dcea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_pipeline.predict(X_train2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = accuracy_score(y_train2, y_pred)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373816e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train2, y_pred, normalize = 'true')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a621de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True,  cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194fd4a",
   "metadata": {},
   "source": [
    "## What is the accuracy score telling us?\n",
    "## What is the confusion matrix telling us? \n",
    "## Which target is classified the best? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29d312",
   "metadata": {},
   "source": [
    "# To DO 7\n",
    "1. Get the best parameters from the best model using the code below\n",
    "2. Get the features as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the best set of parameters found by GridSearchCV\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best parameters found by GridSearchCV:\", best_parameters)\n",
    "\n",
    "# Access the best estimator directly\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc01caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the feature selection step ('anova' in your case)\n",
    "feature_selection_step = best_model.named_steps['anova']\n",
    "# Get the mask of selected features (boolean array)\n",
    "selected_features_mask = feature_selection_step.get_support()\n",
    "selected_columns = X_train.columns[selected_features_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54188dd7",
   "metadata": {},
   "source": [
    "## What features best describe the targets?\n",
    "## How are the features similar or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43c791",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Now we are going to try to predict the SSB intake at year 2 using the features from year 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6354f",
   "metadata": {},
   "source": [
    "# To DO 8\n",
    "1. Subset the X_test data so that it contains only the columns selected from the feature selection above call this X_regression\n",
    "2. Create a X_trainReg, X_testReg, y_trainReg, y_testReg from the X_regression and y_test\n",
    "3. Run a binary logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "# Fit the model on the training data\n",
    "model.fit(X_trainReg, y_trainReg)\n",
    "# Predict on the test set\n",
    "y_predReg = model.predict(X_testReg)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracyReg = accuracy_score(y_testReg, y_predReg)\n",
    "print(f\"Accuracy: {accuracyReg}\")\n",
    "\n",
    "# Coefficients\n",
    "coefficients = model.coef_\n",
    "# Intercepts\n",
    "intercepts = model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8aec8f",
   "metadata": {},
   "source": [
    "# To DO 9\n",
    "1. Make a dataframe of the coefficeients and have the features at the columns names\n",
    "2. Add an intercept column\n",
    "3. Plot the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a033dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(coeff_df.iloc[:, :-1], annot=False, cmap='coolwarm')  # Exclude intercepts for visualization\n",
    "plt.title('Coefficients of Multinomial Logistic Regression')\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd13adc",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We now have done a basic logistic regression, but we want to see what features are signigicant predictors of SSB intake. We will use a different logistic regression package through statsmodels to get more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_trainReg and y_trainReg are your training data and labels\n",
    "# Add constant to the features for the intercept\n",
    "X_test_reg_sm = sm.add_constant(X_testReg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cfbca",
   "metadata": {},
   "source": [
    "# To DO 10\n",
    "1. Replace 2 with 1 in the y_testReg target\n",
    "2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multinomial logistic regression model\n",
    "logit_model = sm.Logit(y_testReg, X_test_reg_sm)\n",
    "result = logit_model.fit()\n",
    "# Summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09127a",
   "metadata": {},
   "source": [
    "## What features are significantly related to SSB intake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f50703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = result.predict(X_test_reg_sm)\n",
    "# Converting probabilities to class labels\n",
    "class_predictions = np.where(predictions > 0.5, 1, 0)\n",
    "# The real target assignments\n",
    "real = y_testReg.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e660131",
   "metadata": {},
   "source": [
    "# To DO 11\n",
    "1. Make a dataframe with two columns 'real' (the actual target classes) and 'pred' (the predicted target classes)\n",
    "2. Use the jaccard score to measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = jaccard_score(dfPrevReal['real'], dfPrevReal['pred'])\n",
    "print(\"Jaccard Similarity Score:\", jaccard)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
