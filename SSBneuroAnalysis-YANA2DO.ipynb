{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fc1134",
   "metadata": {},
   "source": [
    "# SSIB analysis walk through\n",
    "## To DO 0\n",
    "1. Create a git repo\n",
    "2. After each to do, commit and push your changes to the repo (I am going to look at this analysis using the git repo, I should see updates in this notebook at each commit corresponding to the to do)\n",
    "3. Add the Rmd to the git repo as well. THE GIT REPO SHOULD NOT HAVE ANY DATA IN IT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157991cb",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Features\n",
    "The X dataframe is your \"features\". Right now there are too many features for the number of subjects. A good rule of thumb is your features should be 10% or less than your number of samples (subjects). Since we are going to be doing training and testing, we need to limit the number of features to our final testing dataset\n",
    "## Samples\n",
    "This is the number of subjects in our dataset\n",
    "## Targets\n",
    "This is what we are trying to predict. Right now we are trying to predict the high and low SSB groups based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  scipy.signal.signaltools\n",
    "\n",
    "def _centered(arr, newsize):\n",
    "    # Return the center newsize portion of the array.\n",
    "    newsize = np.asarray(newsize)\n",
    "    currsize = np.array(arr.shape)\n",
    "    startind = (currsize - newsize) // 2\n",
    "    endind = startind + newsize\n",
    "    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n",
    "    return arr[tuple(myslice)]\n",
    "\n",
    "scipy.signal.signaltools._centered = _centered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import rankdata, ttest_rel, ttest_1samp\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nibabel as nib\n",
    "#from nilearn.input_data import NiftiLabelsMasker\n",
    "#I got a warning that nilearn.input_data is deprecated, so I imported from nilearn.maskers instead\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.plotting import plot_glass_brain, plot_stat_map, view_img, view_img_on_surf\n",
    "\n",
    "from nltools.data import Brain_Data, Adjacency\n",
    "from nltools.mask import roi_to_brain, expand_mask\n",
    "from nltools.stats import fdr, threshold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa12aca",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Change the path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/Users/gracer/Library/CloudStorage/OneDrive-UniversityofWyoming/0. Lab/M2AENAD Lab - Documents/RESEARCH/ABCD/Yana_SSIB_2024/'\n",
    "#Yana windows\n",
    "#basepath = r'C:\\Users\\Yanko\\OneDrive - University of Wyoming\\Desktop - Copy\\Lab\\SSIB 2024'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ddc2b9",
   "metadata": {},
   "source": [
    "^ Why are you working off the OneDrive :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f653f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(basepath,'data','matchedFinal.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78721282",
   "metadata": {},
   "source": [
    "# To DO 1\n",
    "1. Get the number of subjects call them 'n'\n",
    "## REDO 1\n",
    "1. Use shape to get the number of subjects without hard coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. gettig the n of subjects\n",
    "# df.describe\n",
    "# This isn't a great way to the number of subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba11d4",
   "metadata": {},
   "source": [
    "2. Get a list of ROIs that include all the brain regions, sex, bmi_percentile, household income, and age  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caebe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for columns location to gather them into list later\n",
    "rois_0 = list(df.columns)\n",
    "\n",
    "for i in rois_0:\n",
    "    print(i, df.columns.get_loc(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca4e47",
   "metadata": {},
   "source": [
    "**Call this list of ROIs \"ROIS\"**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_0 = list(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5322c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns needed: 1,3,5:421,425\n",
    "#Don't need the subject keys!\n",
    "ROIS = []\n",
    "ROIS.append(rois_0[1])\n",
    "ROIS.append(rois_0[3])\n",
    "for i in rois_0[5:422]: #here need 422 to get 421 included! \n",
    "    ROIS.append(i)\n",
    "    \n",
    "# How can you do this without a loop?\n",
    "\n",
    "ROIS.append(rois_0[425])\n",
    "ROIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35673718",
   "metadata": {},
   "source": [
    "3. Set sex to 0 for M and 1 for female  \n",
    "4. Set ssb groups to low =0, medium = 1, and high = 2  \n",
    "5. Drop the medium group  \n",
    "6. Create a dataframe called 'X' that is a subset of the dataframe with only the columns in the ROIS list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Set sex to 0 for M and 1 for female  \n",
    "df.replace({'sex': {'F': 1, 'M': 0}}, inplace=True)\n",
    "\n",
    "#4. Set ssb groups to low =0, medium = 1, and high = 2  \n",
    "df.replace({'ssb_group': {'low': 0, 'high': 2}}, inplace=True)\n",
    "\n",
    "#5. Drop the medium group  \n",
    "df = df[df['ssb_group'] != \"medium\"]\n",
    "\n",
    "#6. Create a dataframe called 'X' that is a subset of the dataframe with only the columns in the ROIS list.\n",
    "X = df[ROIS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ab41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transpose the DataFrame to make columns become rows\n",
    "X_T = X.T\n",
    "duplicates = X_T.duplicated(keep='first')\n",
    "# Identifying columns to drop (all duplicates except the first occurrence)\n",
    "cols_to_drop = X_T[duplicates].index\n",
    "# Drop the duplicate columns from the original DataFrame\n",
    "X_cleaned = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "print(X_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just a checkpoint to test for mistakes\n",
    "X_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e779d",
   "metadata": {},
   "source": [
    "## Why do we have to do the step above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28083e82",
   "metadata": {},
   "source": [
    "I am not sure why it's done this way, but I know the purpose of this actions. <br>\n",
    "The model does not take anything except numbers, and we need a df of just numbers to make it run (we could include non-numerical cols (say, IDs) as index, but this would lead to extra changes in Grace's original script and hours of wasted time.) <br>\n",
    "I don't know where duplicates come from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0f62d",
   "metadata": {},
   "source": [
    "# Where do the duplicates come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a01a5",
   "metadata": {},
   "source": [
    "# To DO 2\n",
    "1. Create a dataframe 'y' with only the targets\n",
    "2. Check the number of each target group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a dataframe 'y' with only the targets\n",
    "y = pd.to_numeric(df['ssb_group'])\n",
    "#needed to convert to numeric because it has an object type by default\n",
    "\n",
    "# 2. Check the number of each target group\n",
    "target_counts = y.value_counts()\n",
    "print(\"Number of subjects in each target group:\")\n",
    "print(target_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcd96a",
   "metadata": {},
   "source": [
    "# Train and test datasets\n",
    "Get randomly generated train and test datasets\n",
    "- Train 1 = train the model and feature elimination\n",
    "- Train 2 = cross validate the model\n",
    "- Test = test statistical differences\n",
    "-- In Test we will also have trainReg and testReg \n",
    "-- We need to train and test the signifance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f427da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train, y_train, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59d890",
   "metadata": {},
   "source": [
    "## What proportion of data is in:\n",
    "1. X_train1?\n",
    "2. X_train2?\n",
    "3. X_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proportions of data in each dataset\n",
    "proportion_X_train1 = len(X_train1) / len(X_cleaned)\n",
    "proportion_X_train2 = len(X_train2) / len(X_cleaned)\n",
    "proportion_X_test = len(X_test) / len(X_cleaned)\n",
    "\n",
    "print(\"Proportion of data in X_train1:\", proportion_X_train1)\n",
    "print(\"Proportion of data in X_train2:\", proportion_X_train2)\n",
    "print(\"Proportion of data in X_test:\", proportion_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f22c65",
   "metadata": {},
   "source": [
    "X_cleaned is a pandas df, I have no idea why len works to find the number of rows in a df, when it's a function to tell the length of a list. But it really works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed789095",
   "metadata": {},
   "source": [
    "# To DO 3\n",
    "Now we are going run the model. We are going to use an anova filter with a SVC linear kernel. \n",
    "1. Run the pipeline with the following parameters\n",
    "- Make a list of anova__K parameters with a maximum value that is 10% of our X_test sample in intervals of 10 <br>\n",
    "For example if our max was 1000 we would have a list from 10 to 1000 by 10 (10, 20, 30...1000)\n",
    "-- call this list 'ANOVAK'\n",
    "- Make a list of svc__C parameters that include 0.1, 1, 10, 100\n",
    "-- call this list 'svcC'\n",
    "2. With your lists run the pipeline below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Define the maximum value for ANOVA K parameter (10% of X_test sample)\n",
    "max_anova_k = int(len(X_test) * 0.1) #made it an integer, cause it's 92.7 and float can't be used in the script below\n",
    "\n",
    "# Make a list of ANOVA K parameters with intervals of 10\n",
    "ANOVAK = list(range(10, max_anova_k+1, 10)) #range 10-max_anova_k (+1 bcs otherwise the last number won't be included to the range), steps - 10\n",
    "ANOVAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f876f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Make a list of svc__C parameters that include 0.1, 1, 10, 100\n",
    "#call this list 'svcC'\n",
    "svcC = [0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49dca5e",
   "metadata": {},
   "source": [
    "## Why are we using an ANOVA filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1511f41",
   "metadata": {},
   "source": [
    "To select features based on whether they are significant or not. <br>\n",
    "These features will be used as predictor values in our model to classify the ssb group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485cb53",
   "metadata": {},
   "source": [
    "## Why use the ANOVA and not any other type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS THE PIPELINE ##\n",
    "\n",
    "anova_filter = SelectKBest(f_classif)\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "anova_svm = Pipeline([\n",
    "    ('anova', anova_filter),\n",
    "    ('svc', svm)\n",
    "])\n",
    "# Define a range of parameters for feature selection and SVM\n",
    "param_grid = {\n",
    "    'anova__k': ANOVAK,  # Trying different numbers of top features\n",
    "    'svc__C': svcC,  # SVM regularization parameter\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(anova_svm, param_grid=param_grid, cv=10, n_jobs=4)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eed021",
   "metadata": {},
   "source": [
    "# To DO 4\n",
    "1. Look at the \"best parameters\". Based on this narrow your ANOVAK and svcC lists. The ANOVAK should have the best value and the 10 digits around it. For example if the best value was 20, the next ANOVAK list should be [15,16,17,18,19,20,21,22,23,24,25]. Do the same with the best value of svcC.\n",
    "2. Run the loop below (this will take a while) with your new parameters\n",
    "## What is the optimum number of features and what is the optimum svc C based on your loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e012da",
   "metadata": {},
   "source": [
    "Best parameters: {'anova__k': 60, 'svc__C': 0.1}\n",
    "\n",
    "Best cross-validation score: 0.5510857563489142\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c071af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOVAK = list(range(55, 66, 1)) #range 55-65, steps - 1\n",
    "print(ANOVAK)\n",
    "print(len(ANOVAK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee05acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the range function does not work with integers, so I created a loop\n",
    "# The range function only works with integers, it doesn't work with floats\n",
    "#and cleaned the svcC list to fill with necessary numbers\n",
    "svcC = []\n",
    "for i in range(75, 126, 5):\n",
    "    svcC.append(i/1000.0)\n",
    "print(svcC)\n",
    "print(len(svcC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc6244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a range of parameters for feature selection and SVM\n",
    "param_grid = {\n",
    "    'anova__k': ANOVAK,  # Trying different numbers of top features\n",
    "    'svc__C': svcC,  # SVM regularization parameter\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(anova_svm, param_grid=param_grid, cv=10, n_jobs=4)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "bestK = []\n",
    "bestC = []\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    # Setup GridSearchCV\n",
    "    grid_search = GridSearchCV(anova_svm, param_grid=param_grid, cv=10, n_jobs=4)\n",
    "    grid_search.fit(X_train1, y_train1)\n",
    "    bestK.append(grid_search.best_params_['anova__k'])\n",
    "    bestC.append(grid_search.best_params_['svc__C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca39e0e",
   "metadata": {},
   "source": [
    "# To DO 5\n",
    "1. Using the best parameters run the code below \n",
    "## Are these the best parameters or just the last parameters in from the best_pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0caecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming anova_svm is your original pipeline\n",
    "best_pipeline = Pipeline([\n",
    "    ('anova', SelectKBest(f_classif)),  # You don't need to specify k here; it will be set by best_params_\n",
    "    ('svc', SVC(kernel='linear'))      # No need to specify C here for the same reason\n",
    "])\n",
    "\n",
    "# Set the best parameters found for the entire pipeline\n",
    "best_pipeline.set_params(**grid_search.best_params_)\n",
    "\n",
    "# Now, retrain on the entire training set with the best parameters\n",
    "best_pipeline.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d3cee",
   "metadata": {},
   "source": [
    "# To DO 6\n",
    "1. Get the accuracy score\n",
    "2. Look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dcea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_pipeline.predict(X_train2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = accuracy_score(y_train2, y_pred)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373816e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train2, y_pred, normalize = 'true')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a621de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True,  cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194fd4a",
   "metadata": {},
   "source": [
    "## What is the accuracy score telling us?\n",
    "\n",
    "The accuracy score is 0.5330459770114943. <br> \n",
    "It means that our model predicts correctly in 53.3% cases. <br>\n",
    "Still, it's considered not the perfect indicator of the model performance (especially if the groups are unbalanced, which I believe is our case, considering how we had to drop the bmi% groups + we have household income groups, which also probably aren't very balanced.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13815696",
   "metadata": {},
   "source": [
    "The groups are only refering to the target groups. Also our data is pretty well matched. So why isn't the model that accurate? "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "76f91054",
   "metadata": {},
   "source": [
    "## What is the confusion matrix telling us? \n",
    "The confusion matrix is a matrix of numbers that tell us where a model gets confused. Say, of 4 classes, what if the model is predicting 1 class wrong and that's driving the accuracy score down? <br> \n",
    "So, confusion matrix gives a more detailed breakdown of the model's performance, showing how many instances were correctly or incorrectly classified, and in what way. Here's what each part of the matrix is telling us:\n",
    "\n",
    "- **<u>0-0 True Negative (Top Left, 0.76)</u>**: The proportion of actual negatives (low ssb) that were correctly identified by the model. 76% is a pretty good number, it means that the model is relatively good at identifying the low ssb group.\n",
    "- **<u> 1-0 False Positive (Top Right, 0.24)</u>**: The proportion of actual negatives (low ssb) that were incorrectly identified as positives (high ssb). This is a type I error.\n",
    "- **<u> 0-1 False Negative (Bottom Left, 0.67)</u>**: The proportion of actual positives (high ssb) that were incorrectly identified as negatives (low ssb). This is a type II error and is quite high, indicating that the model often fails to identify positive instances (high ssb).\n",
    "- **<u> 1-1 True Positive (Bottom Right, 0.33)</u>**: The proportion of actual positives (high ssb) that were correctly identified by the model. <br> 33% value is low, which means that our model is not very effective at correctly identifying positive instances (high ssb).\n",
    "\n",
    "## 0s - low ssb group, 1s - high ssb group \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610d0cd",
   "metadata": {},
   "source": [
    "## Which target is classified the best? \n",
    "\n",
    "Low ssb groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29d312",
   "metadata": {},
   "source": [
    "# To DO 7\n",
    "1. Get the best parameters from the best model using the code below\n",
    "2. Get the features as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the best set of parameters found by GridSearchCV\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best parameters found by GridSearchCV:\", best_parameters)\n",
    "\n",
    "# Access the best estimator directly\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc01caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the feature selection step ('anova' in your case)\n",
    "feature_selection_step = best_model.named_steps['anova']\n",
    "# Get the mask of selected features (boolean array)\n",
    "selected_features_mask = feature_selection_step.get_support()\n",
    "selected_columns = X_train.columns[selected_features_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54188dd7",
   "metadata": {},
   "source": [
    "## What features best describe the targets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e321bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ce10c",
   "metadata": {},
   "source": [
    "## How are the features similar or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0062f5c",
   "metadata": {},
   "source": [
    "They are all brain regions, there's no bmi or ses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43c791",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Now we are going to try to predict the SSB intake at year 2 using the features from year 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6354f",
   "metadata": {},
   "source": [
    "# To DO 8\n",
    "1. Subset the X_test data so that it contains only the columns selected from the feature selection above call this X_regression\n",
    "2. Create a X_trainReg, X_testReg, y_trainReg, y_testReg from the X_regression and y_test\n",
    "3. Run a binary logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Subset the X_test data to contain only the columns selected from the feature selection\n",
    "X_regression = X_test[selected_columns]\n",
    "\n",
    "#2. Create a X_trainReg, X_testReg, y_trainReg, y_testReg from the X_regression and y_test\n",
    "X_trainReg, X_testReg, y_trainReg, y_testReg = train_test_split(X_regression, y_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "# Fit the model on the training data\n",
    "model.fit(X_trainReg, y_trainReg)\n",
    "# Predict on the test set\n",
    "y_predReg = model.predict(X_testReg)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracyReg = accuracy_score(y_testReg, y_predReg)\n",
    "print(f\"Accuracy: {accuracyReg}\")\n",
    "\n",
    "# Coefficients\n",
    "coefficients = model.coef_\n",
    "# Intercepts\n",
    "intercepts = model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8aec8f",
   "metadata": {},
   "source": [
    "# To DO 9\n",
    "1. Make a dataframe of the coefficeients and have the features at the columns names\n",
    "2. Add an intercept column\n",
    "3. Plot the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df66fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make a dataframe of the coefficients and have the features as the column names\n",
    "coeff_df = pd.DataFrame(coefficients, columns=X_regression.columns)\n",
    "\n",
    "# 2. Add an intercept column\n",
    "coeff_df['intercept'] = intercepts\n",
    "\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a033dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(coeff_df.iloc[:, :-1], annot=False, cmap='coolwarm')  # Exclude intercepts for visualization\n",
    "plt.title('Coefficients of Multinomial Logistic Regression')\n",
    "plt.ylabel('Class')\n",
    "plt.xlabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd13adc",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We now have done a basic logistic regression, but we want to see what features are signigicant predictors of SSB intake. We will use a different logistic regression package through statsmodels to get more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_trainReg and y_trainReg are your training data and labels\n",
    "# Add constant to the features for the intercept\n",
    "X_test_reg_sm = sm.add_constant(X_testReg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cfbca",
   "metadata": {},
   "source": [
    "# To DO 10\n",
    "1. Replace 2 with 1 in the y_testReg target\n",
    "2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50952e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Replace 2 with 1 in the y_testReg target\n",
    "y_testReg = y_testReg.replace(2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d1f20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the multinomial logistic regression model\n",
    "logit_model = sm.Logit(y_testReg, X_test_reg_sm)\n",
    "result = logit_model.fit()\n",
    "# Summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08551f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09127a",
   "metadata": {},
   "source": [
    "## What features are significantly related to SSB intake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a30194",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05]\n",
    "print(significant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f50703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = result.predict(X_test_reg_sm)\n",
    "# Converting probabilities to class labels\n",
    "class_predictions = np.where(predictions > 0.5, 1, 0)\n",
    "# The real target assignments\n",
    "real = y_testReg.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e660131",
   "metadata": {},
   "source": [
    "# To DO 11\n",
    "1. Make a dataframe with two columns 'real' (the actual target classes) and 'pred' (the predicted target classes)\n",
    "2. Use the jaccard score to measure the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db312fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with two columns 'real' and 'pred'\n",
    "dfPrevReal = pd.DataFrame({\n",
    "    'real': real,\n",
    "    'pred': class_predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = jaccard_score(dfPrevReal['real'], dfPrevReal['pred'])\n",
    "print(\"Jaccard Similarity Score:\", jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get getting the odds ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458fc69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.exp(result.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed668cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05815daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multinomial logistic regression model\n",
    "#logit_model = sm.Logit(y_testReg, X_test_reg_sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2a391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_test_reg_sm, y_testReg)\n",
    "\n",
    "df=pd.DataFrame({'odds_ratio':(np.exp(logisticRegr.coef_).T).tolist(),'variable':X_test_reg_sm.columns.tolist()})\n",
    "df['odds_ratio'] = df['odds_ratio'].str.get(0)\n",
    "\n",
    "df=df.sort_values('odds_ratio', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daedc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "coef_matrix = []\n",
    "\n",
    "# Bootstrap loop\n",
    "for _ in range(n_bootstraps):\n",
    "    X_sample, y_sample = resample(X_test_reg_sm, y_testReg)\n",
    "    logisticRegr.fit(X_sample, y_sample)\n",
    "    coef_matrix.append(logisticRegr.coef_[0])\n",
    "\n",
    "# Convert to numpy array for ease of calculation\n",
    "coef_matrix = np.array(coef_matrix)\n",
    "\n",
    "# Calculating percentiles for 95% confidence intervals\n",
    "lower_bounds = np.percentile(coef_matrix, 2.5, axis=0)\n",
    "upper_bounds = np.percentile(coef_matrix, 97.5, axis=0)\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with your coefficients and variable names\n",
    "df['lower_ci'] = lower_bounds\n",
    "df['upper_ci'] = upper_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a8283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af3550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_with_const = sm.add_constant(X_test_reg_sm)  # Adding a constant for the intercept\n",
    "logit_model = sm.Logit(y_testReg, X_with_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Extracting the p-values\n",
    "p_values = result.pvalues\n",
    "\n",
    "# Extracting the confidence intervals\n",
    "conf_intervals = result.conf_int()\n",
    "\n",
    "# Adding p-values and confidence intervals to your DataFrame\n",
    "df_stats = pd.DataFrame({'variable': X_with_const.columns, 'coef': result.params, 'p_value': p_values})\n",
    "df_stats['lower_ci'], df_stats['upper_ci'] = conf_intervals[0], conf_intervals[1]\n",
    "\n",
    "print(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73604904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization check\n",
    "logisticRegr = LogisticRegression(C=1e9, solver='lbfgs', max_iter=1000)\n",
    "# or for no regularization in newer versions\n",
    "# logisticRegr = LogisticRegression(penalty='none', solver='lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33649fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_test_reg_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reg_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3a701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_scaled, y_testReg)\n",
    "\n",
    "df=pd.DataFrame({'odds_ratio':(np.exp(logisticRegr.coef_).T).tolist(),'variable':X_test_reg_sm.columns.tolist()})\n",
    "df['odds_ratio'] = df['odds_ratio'].str.get(0)\n",
    "\n",
    "#df=df.sort_values('odds_ratio', ascending=False)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9a54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd6d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the multinomial logistic regression model\n",
    "logit_model = sm.Logit(y_testReg, X_scaled)\n",
    "result = logit_model.fit()\n",
    "# Summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(result.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc58203",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05]\n",
    "print(significant_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6b2be",
   "metadata": {},
   "source": [
    "# Grace clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94df53",
   "metadata": {},
   "source": [
    "## Checking odds ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2bc17",
   "metadata": {},
   "source": [
    "### Prepro check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb27f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_testReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a1251",
   "metadata": {},
   "source": [
    "### StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ffac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_trainReg and y_trainReg are your training data and labels\n",
    "# Add constant to the features for the intercept\n",
    "X_test_scaled_sm = sm.add_constant(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de077f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the multinomial logistic regression model\n",
    "logit_model = sm.Logit(y_testReg, X_test_scaled_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee169326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model with regularization\n",
    "# Here we demonstrate using L1 regularization (Lasso)\n",
    "# You can adjust L1 vs. L2 regularization via the L1_wt parameter (0 for L2, 1 for L1)\n",
    "# alpha controls the strength of regularization (smaller values indicate stronger regularization)\n",
    "result_regularized = logit_model.fit_regularized(method='l1', alpha=1, L1_wt=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b8afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "coef = result_regularized.params\n",
    "print(\"Coefficients:\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(X_scaled, columns=X_test_reg_sm.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed31d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variance = df_scaled.var()\n",
    "print(variance)  # Low variance columns might be problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d3447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming X_scaled is your scaled feature matrix and it's a numpy array\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_test_reg_sm.columns)  # Convert to DataFrame if necessary\n",
    "X_scaled_df_with_const = sm.add_constant(X_scaled_df)  # Add constant for VIF calculation\n",
    "\n",
    "vifs = pd.Series([variance_inflation_factor(X_scaled_df_with_const.values, i) \n",
    "                   for i in range(X_scaled_df_with_const.shape[1])], \n",
    "                  index=X_scaled_df_with_const.columns)\n",
    "\n",
    "print(vifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_scaled_df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c46337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for high correlation\n",
    "threshold = 0.8\n",
    "\n",
    "# Create a boolean mask for values above the threshold\n",
    "high_corr_mask = np.abs(correlation_matrix) > threshold\n",
    "\n",
    "# Mask the diagonal and lower triangle\n",
    "mask_upper_triangle = np.triu(np.ones(high_corr_mask.shape), k=1).astype(np.bool)\n",
    "\n",
    "# Combine masks\n",
    "final_mask = high_corr_mask & mask_upper_triangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mask to the correlation matrix\n",
    "high_corr_pairs = correlation_matrix.where(final_mask)\n",
    "\n",
    "# Stack the matrix and reset index to get pair-wise correlation in a readable format\n",
    "stacked_corr_pairs = high_corr_pairs.stack().reset_index()\n",
    "stacked_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "\n",
    "print(stacked_corr_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8ebe1",
   "metadata": {},
   "source": [
    "Need to drop highly correlated columns\n",
    "* rsfmri_cor_ngd_au_scs_ptrh = Average correlation between auditory network and ASEG ROI right-putamen\n",
    "* rsfmri_cor_ngd_rst_scs_crcxrh = Average correlation between retrosplenial temporal network and ASEG ROI right-cerebellum-cortex\n",
    "* rsfmri_cor_ngd_cerc_scs_aglh = Average correlation between cingulo-opercular network and ASEG ROI left-amygdala\n",
    "* rsfmri_cor_ngd_smh_scs_pllh = Average correlation between sensorimotor hand network and ASEG ROI left-pallidum\n",
    "* rsfmri_cor_ngd_cerc_scs_hprh = Average correlation between cingulo-opercular network and ASEG ROI right-hippocampus\n",
    "* rsfmri_cor_ngd_smh_scs_cderh = Average correlation between sensorimotor hand network and ASEG ROI right-caudate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642d41b",
   "metadata": {},
   "source": [
    "Based on this list above we have some interest high correlations. I want to keep areas previously associated in the literature. \n",
    "## Keep\n",
    "* rsfmri_cor_ngd_au_scs_ptrh = Average correlation between auditory network and ASEG ROI right-putamen\n",
    "* rsfmri_cor_ngd_smh_scs_cderh = Average correlation between sensorimotor hand network and ASEG ROI right-caudate\n",
    "* rsfmri_cor_ngd_smh_scs_pllh = Average correlation between sensorimotor hand network and ASEG ROI left-pallidum <- chose to keep since I dropped the other sensorimotor hand\n",
    "## Drop\n",
    "* rsfmri_cor_ngd_rst_scs_crcxrh = Average correlation between retrosplenial temporal network and ASEG ROI right-cerebellum-cortex\n",
    "* rsfmri_cor_ngd_cerc_scs_hprh = Average correlation between cingulo-opercular network and ASEG ROI right-hippocampus <- this was a tough one, may do the analysis with both at some point\n",
    "* rsfmri_cor_ngd_cerc_scs_aglh = Average correlation between cingulo-opercular network and ASEG ROI left-amygdala\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df730a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_list = ['rsfmri_cor_ngd_rst_scs_crcxrh','rsfmri_cor_ngd_cerc_scs_hprh','rsfmri_cor_ngd_cerc_scs_aglh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e857287",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df_reduced = X_scaled_df.drop(columns=columns_to_drop_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba0fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming X_scaled is your scaled feature matrix and it's a numpy array\n",
    "#X_scaled_df = pd.DataFrame(X_scaled, columns=X_test_reg_sm.columns)  # Convert to DataFrame if necessary\n",
    "#X_scaled_df_with_const = sm.add_constant(X_scaled_df)  # Add constant for VIF calculation\n",
    "\n",
    "vifs = pd.Series([variance_inflation_factor(X_scaled_df_reduced.values, i) \n",
    "                   for i in range(X_scaled_df_reduced.shape[1])], \n",
    "                  index=X_scaled_df_reduced.columns)\n",
    "\n",
    "print(vifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bb748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f993b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43105c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e583db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb65a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_reduced is your DataFrame after removing highly correlated columns\n",
    "# and y_testReg is your target variable\n",
    "\n",
    "# Reset index for the predictors (features)\n",
    "X_aligned = X_scaled_df_reduced.reset_index(drop=True)\n",
    "\n",
    "# Reset index for the target variable\n",
    "y_aligned = y_testReg.reset_index(drop=True)\n",
    "\n",
    "# Now, both X_aligned and y_aligned have aligned indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multinomial logistic regression model\n",
    "logit_model = sm.Logit(y_aligned, X_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51670f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with regularization\n",
    "# Here we demonstrate using L1 regularization (Lasso)\n",
    "# You can adjust L1 vs. L2 regularization via the L1_wt parameter (0 for L2, 1 for L1)\n",
    "# alpha controls the strength of regularization (smaller values indicate stronger regularization)\n",
    "result_regularized = logit_model.fit_regularized(method='l1', alpha=1, L1_wt=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e14a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "coef = result_regularized.params\n",
    "print(\"Coefficients:\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f4a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8932e6bd",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96087e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "#y_aligned, X_aligned\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_aligned, y_aligned)\n",
    "\n",
    "df=pd.DataFrame({'odds_ratio':(np.exp(logisticRegr.coef_).T).tolist(),'variable':X_aligned.columns.tolist()})\n",
    "df['odds_ratio'] = df['odds_ratio'].str.get(0)\n",
    "\n",
    "df=df.sort_values('odds_ratio', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "coef_matrix = []\n",
    "#y_aligned, X_aligned\n",
    "# Bootstrap loop\n",
    "for _ in range(n_bootstraps):\n",
    "    X_sample, y_sample = resample(X_aligned, y_aligned)\n",
    "    logisticRegr.fit(X_sample, y_sample)\n",
    "    coef_matrix.append(logisticRegr.coef_[0])\n",
    "\n",
    "# Convert to numpy array for ease of calculation\n",
    "coef_matrix = np.array(coef_matrix)\n",
    "\n",
    "# Calculating percentiles for 95% confidence intervals\n",
    "lower_bounds = np.percentile(coef_matrix, 2.5, axis=0)\n",
    "upper_bounds = np.percentile(coef_matrix, 97.5, axis=0)\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with your coefficients and variable names\n",
    "df['lower_ci'] = lower_bounds\n",
    "df['upper_ci'] = upper_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961477e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(significant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2609588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
